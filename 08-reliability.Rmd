`r if (knitr:::is_html_output()) '
# Reliability
'`
## Introduction
This section introduces the terminology and models that will be used to describe and quantify product reliability. The terminology, probability distributions and models used for reliability analysis differ in many cases from those used in other statistical applications.

### Why is the assessment and control of product reliability important?
>We depend on, demand, and expect reliable products	

In today's technological world nearly everyone depends upon the continued functioning of a wide array of complex machinery and equipment for their everyday health, safety, mobility and economic welfare. We expect our cars, computers, electrical appliances, lights, televisions, etc. to function whenever we need them - day after day, year after year. When they fail the results can be catastrophic: injury, loss of life and/or costly lawsuits can occur. More often, repeated failure leads to annoyance, inconvenience and a lasting customer dissatisfaction that can play havoc with the responsible company's marketplace position. 

>Shipping unreliable products can destroy a company's reputation	

It takes a long time for a company to build up a reputation for reliability, and only a short time to be branded as "unreliable" after shipping a flawed product. Continual assessment of new product reliability and ongoing control of the reliability of everything shipped are critical necessities in today's competitive business arena. 

#### Quality versus reliability

>Reliability is "quality changing over time" 	

The everyday usage term "quality of a product" is loosely taken to mean its inherent degree of excellence. In industry, this is made more precise by defining quality to be "conformance to requirements at the start of use". Assuming the product specifications adequately capture customer requirements, the quality level can now be precisely measured by the fraction of units shipped that meet specifications.

>A motion picture instead of a snapshot	

But how many of these units still meet specifications after a week of operation? Or after a month, or at the end of a one year warranty period? That is where "reliability" comes in. Quality is a snapshot at the start of life and reliability is a motion picture of the day-by-day operation. Time zero defects are manufacturing mistakes that escaped final test. The additional defects that appear over time are "reliability defects" or reliability fallout.

> Life distributions model fraction fallout over time	

The quality level might be described by a single fraction defective. To describe reliability fallout a probability model that describes the fraction fallout over time is needed. This is known as the life distribution model.

#### Competitive driving factors
> Reliability is a major economic factor in determining a product's success	

Accurate prediction and control of reliability plays an important role in the profitability of a product. Service costs for products within the warranty period or under a service contract are a major expense and a significant pricing factor. Proper spare part stocking and support personnel hiring and training also depend upon good reliability fallout predictions. On the other hand, missing reliability targets may invoke contractual penalties and cost future business. 
Companies that can economically design and market products that meet their customers' reliability expectations have a strong competitive advantage in today's marketplace. 

#### Safety and health considerations
>Some failures have serious social consequences and this should be taken into account when planning reliability studies	

Sometimes equipment failure can have a major impact on human safety and/or health. Automobiles, planes, life support equipment, and power generating plants are a few examples. 
From the point of view of "assessing product reliability", we treat these kinds of catastrophic failures no differently from the failure that occurs when a key parameter measured on a manufacturing tool drifts slightly out of specification, calling for an unscheduled maintenance action. 

It is up to the reliability engineer (and the relevant customer) to define what constitutes a failure in any reliability study. More resource (test time and test units) should be planned for when an incorrect reliability assessment could negatively impact safety and/or health.

### What are the basic terms and models used for reliability evaluation?
>Reliability methods and terminology began with 19th century insurance companies	

Reliability theory developed apart from the mainstream of probability and statistics, and was used primarily as a tool to help nineteenth century maritime and life insurance companies compute profitable rates to charge their customers. Even today, the terms "failure rate" and "hazard rate" are often used interchangeably. 
The following sections will define some of the concepts, terms, and models we need to describe, estimate and predict reliability.

#### Repairable systems, non-repairable populations and lifetime distribution models
>Life distribution models describe how non-repairable populations fail over time

A repairable system is one which can be restored to satisfactory operation by any action, including parts replacements or changes to adjustable settings. When discussing the rate at which failures occur during system operation time (and are then repaired) we will define a Rate of Occurrence of Failure (ROCF) or "repair rate". It would be incorrect to talk about failure rates or hazard rates for repairable systems, as these terms apply only to the first failure times for a population of non repairable components.
A non-repairable population is one for which individual items that fail are removed permanently from the population. While the system may be repaired by replacing failed units from either a similar or a different population, the members of the original population dwindle over time until all have eventually failed.

We begin with models and definitions for non-repairable populations. Repair rates for repairable populations will be defined in a later section.

The theoretical population models used to describe unit lifetimes are known as **Lifetime Distribution Models**. The population is generally considered to be all of the possible unit lifetimes for all of the units that could be manufactured based on a particular design and choice of materials and manufacturing process. A random sample of size *n* from this population is the collection of failure times observed for a randomly selected group of *n* units.

>Any continuous PDF defined only for non-negative values can be a lifetime distribution model

A lifetime distribution model can be any probability density function (or PDF) f(t) defined over the range of time from t=0,…,∞. The corresponding cumulative distribution function (or CDF) F(t) is a very useful function, as it gives the probability that a randomly selected unit will fail by time t. The figure below shows the relationship between f(t) and F(t) and gives three descriptions of F(t). 

![PDF](http://www.itl.nist.gov/div898/handbook/apr/section1/gifs/f81212.gif)

1. F(t) = the area under the PDF f(t) to the left of t.
2. F(t) = the probability that a single randomly chosen new unit will fail by time t.
3. F(t) = the proportion of the entire population that fails by time t.

The figure above also shows a shaded area under $f(t)$ between the two times $t_1$ and $t_2$. This area is $[F(t2)−F(t1)]$ and represents the proportion of the population that fails between times $t_1$ and $t_2$ (or the probability that a brand new randomly chosen unit will survive to time $t_1$ but fail before time $t_2$).
Note that the PDF $f(t)$ has only non-negative values and eventually either becomes 0 as t increases, or decreases towards 0. The CDF $F(t)$ is monotonically increasing and goes from 0 to 1 as t approaches infinity. In other words, the total area under the curve is always 1.

>The Weibull model is a good example of a life distribution

The 2-parameter Weibull distribution is an example of a popular $F(t)$. It has the CDF and PDF equations given by:
$$F(t)= 1-e^{-(t/\alpha)^\gamma}$$
$$f(t)=\frac{\gamma}{t} \left(\frac{t}{\alpha} \right)^\gamma e^{-(t/\alpha)^\gamma}$$
where $\gamma$ is the "shape" parameter and $\alpha$ is a "scale" parameter called the characteristic life.
Example: A company produces automotive fuel pumps that fail according to a Weibull life distribution model with shape parameter $\gamma=1.5$ and scale parameter 8,000 (time measured in use hours). If a typical pump is used 800 hours a year, what proportion are likely to fail within 5 years?

Solution: The probability associated with the 800*5 quantile of a Weibull distribution with $\gamma=1.5$ and $\alpha=8000$ is `r pweibull(800*5, 1.5, 8000)`. Thus about `r paste0(as.character(round(pweibull(800*5, 1.5, 8000)*100, digits = 0)), "%")` of the pumps will fail in the first 5 years.




Functions for computing PDF values and CDF values, are available in both Dataplot code and R code.
```{r}
print((paste("The probability associated with the 800*5 quantile of a Weibull distribution with gamma=1.5 and alpha=8000 is", as.character(pweibull(800*5, 1.5, 8000)))))
print(paste("Thus about", paste0(as.character(round(pweibull(800*5, 1.5, 8000)*100, digits = 0)), "%"),  "of the pumps will fail in the first 5 years."))
```

#### Reliability or survival function
>Survival is the complementary event to failure

The Reliability Function $R(t)$, also known as the Survival Function $S(t)$, is defined by
R(t)=S(t)=the probability a unit survives beyond time t.
Since a unit either fails, or survives, and one of these two mutually exclusive alternatives must occur, we have
$$R(t)=1−F(t)$$ $$F(t)=1−R(t)$$.

>The reliability of the system is the product of the reliability functions of the components

Calculations using R(t) often occur when building up from single components to subsystems with many components. For example, if one microprocessor comes from a population with reliability function $R_{m}(t)$ and two of them are used for the CPU in a system, then the system CPU has a reliability function given by $$R_{cpu}(t) = R_m^2(t)$$
since both must survive in order for the system to survive. This building up to the system from the individual components will be discussed in detail when we look at the "Bottom-Up" method. The general rule is: to calculate the reliability of a system of independent components, multiply the reliability functions of all the components together.

#### Failure (or hazard) rate

>The failure rate is the rate at which the population survivors at any given instant are "falling over the cliff"

The failure rate is defined for non repairable populations as the (instantaneous) rate of failure for the survivors to time t during the next instant of time. It is a rate per unit of time similar in meaning to reading a car speedometer at a particular instant and seeing 45 mph. The next instant the failure rate may change and the units that have already failed play no further role since only the survivors count. 
The failure rate (or hazard rate) is denoted by h(t) and is calculated from
$$h(t) = \frac{f(t)}{1 - F(t)} = \frac{f(t)}{R(t)} = \mbox{the instantaneous (conditional) failure rate.}$$
The failure rate is sometimes called a "conditional failure rate" since the denominator $1−F(t)$ (i.e., the population survivors) converts the expression into a conditional rate, given survival past time t.
Since $h(t)$ is also equal to the negative of the derivative of $ln[R(t)]$, we have the useful identity:
$$F(t)=1-\mbox{exp}\left[-\int_0^t h(t)dt\right]$$
If we let
$$H(t) = \int_0^t h(t)dt$$

be the Cumulative Hazard Function, we then have $F(t)=1−e^{H(t)}$. Two other useful identities that follow from these formulas are:
$$h(t) = - \frac{d \mbox{ln} R(t)}{dt}$$
$$H(t) = - \mbox{ln} R(t)$$

It is also sometimes useful to define an average failure rate over any interval (T~1~,T~2~) that "averages" the failure rate over that interval. This rate, denoted by AFR(T~1~,T~2~), is a single number that can be used as a specification or target for the population failure rate over that interval. If T~1~ is 0, it is dropped from the expression. Thus, for example, AFR(40,000) would be the average failure rate for the population over the first 40,000 hours of operation. 

The formulas for calculating *AFR* values are:
$$AFR(T_2 - T_1) = \frac{\int_{T_1}^{T_2} h(t)dt}{T_2 - T_1} = \frac{H(T_2) - H(T_1)}{T_2 - T_1} = \frac{\mbox{ln}R(T_1) - \mbox{ln}R(T_2)}{T_2 - T_1}$$
   
and $$AFR(0,T) = AFR(T) = \frac{H(T)}{T} = \frac{-\mbox{ln} R(T)}{T}$$

#### "Bathtub" curve
>A plot of the failure rate over time for most products yields a curve that looks like a drawing of a bathtub

If enough units from a given population are observed operating and failing over time, it is relatively easy to compute week-by-week (or month-by-month) estimates of the failure rate $h(t)$. For example, if $N_{12}$ units survive to start the 13th month of life and $r_{13}$ of them fail during the next month (or 720 hours) of life, then a simple empirical estimate of $h(t)$ averaged across the 13th month of life (or between 8640 hours and 9360 hours of age), is given by $(\frac{r_{13}} {N_{12}}⋅720)$. Similar estimates are discussed in detail in the section on Empirical Model Fitting. 

Over many years, and across a wide variety of mechanical and electronic components and systems, people have calculated empirical population failure rates as units age over time and repeatedly obtained a graph such as shown below. Because of the shape of this failure rate curve, it has become widely known as the "Bathtub" curve. 

The initial region that begins at time zero when a customer first begins to use the product is characterized by a high but rapidly decreasing failure rate. This region is known as the **Early Failure Period** (also referred to as **Infant Mortality Period**, from the actuarial origins of the first bathtub curve plots). This decreasing failure rate typically lasts several weeks to a few months. 

Next, the failure rate levels off and remains roughly constant for (hopefully) the majority of the useful life of the product. This long period of a level failure rate is known as the **Intrinsic Failure Period** (also called the **Stable Failure Period**) and the constant failure rate level is called the **Intrinsic Failure Rate**. Note that most systems spend most of their lifetimes operating in this flat portion of the bathtub curve 

Finally, if units from the population remain in use long enough, the failure rate begins to increase as materials wear out and degradation failures occur at an ever increasing rate. This is the **Wearout Failure Period**. 

![Bathtub Curve](http://www.itl.nist.gov/div898/handbook/apr/section1/gifs/bathtub2.gif)

NOTE: The Bathtub Curve also applies (based on much empirical evidence) to Repairable Systems. In this case, the vertical axis is the Repair Rate or the Rate of Occurrence of Failures (ROCOF). 

#### Repair rate or ROCOF
>Repair Rate models are based on counting the cumulative number of failures over time

A different approach is used for modeling the rate of occurrence of failure incidences for a repairable system. In this chapter, these rates are called repair rates (not to be confused with the length of time for a repair, which is not discussed in this chapter). Time is measured by system power-on-hours from initial turn-on at time zero, to the end of system life. Failures occur as given system ages and the system is repaired to a state that may be the same as new, or better, or worse. The frequency of repairs may be increasing, decreasing, or staying at a roughly constant rate. 
Let *N(t)* be a counting function that keeps track of the cumulative number of failures a given system has had from time zero to time t. *N(t)* is a step function that jumps up one every time a failure occurs and stays at the new level until the next failure. 

Every system will have its own observed *N(t)* function over time. If we observed the *N(t)* curves for a large number of similar systems and "averaged" these curves, we would have an estimate of $M(t) = \mbox{the expected number (average number) of cumulative failures by time t for these systems}$.

>>**The Repair Rate** (or **ROCOF**) is the mean rate of failures per unit time	

The derivative of *M(t)*, denoted *m(t)*, is defined to be the Repair Rate or the **Rate Of Occurrence Of Failures at Time t**, or **ROCOF**.
Models for *N(t)*, *M(t)*, and *m(t)* will be described in the section on Repair Rate Models[insert ].

### What are some common difficulties with reliability data and how are they overcome?
>The Paradox of Reliability Analysis: The more reliable a product is, the harder it is to get the failure data needed to "prove" it is reliable!

There are two closely related problems that are typical with reliability data and not common with most other forms of statistical data. These are: 

- [Censoring](insert)(when the observation period ends, not all units have failed - some are survivors)
- [Lack of Failures](insert) (if there is too much censoring, even though a large number of units may be under observation, the information in the data is limited due to the lack of actual failures)

These problems cause considerable practical difficulty when planning reliability assessment tests and analyzing failure data. Some solutions are discussed in the next two sections. Typically, the solutions involve making additional assumptions and using complicated models.

#### Censoring
>When not all units on test fail we have censored data

Consider a situation in which we are reliability testing n (non-repairable) units taken randomly from a population. We are investigating the population to determine if its failure rate is acceptable. In the typical test scenario, we have a fixed time T to run the units to see if they survive or fail. The data obtained are called **Censored Type I data**.

##### Censored Type I Data

During the T hours of test we observe r failures (where r can be any number from 0 to n). The (exact) failure times are *t~1~,t~2~,…,t~r~*, and there are *(n−r)* units that survived the entire *T-hour* test without failing. Note that *T* is fixed in advance and r is random, since we don't know how many failures will occur until the test is run. Note also that we assume the exact times of failure are recorded when there are failures. 

This type of censoring is also called "right censored" data since the times of failure to the right (i.e., larger than *T*) are missing.

Another (much less common) way to test is to decide in advance that you want to see exactly r failure times and then test until they occur. For example, you might put 100 units on test and decide you want to see at least half of them fail. Then r=50, but T is unknown until the 50th failure occurs. This is called **Censored Type II data.**

##### Censored Type II Data

We observe *t~1~,t~2~,…,t~r~*, where *r* is specified in advance. The test ends at time *T=t~r~*, and *(n−r)* units have survived. Again we assume it is possible to observe the exact time of failure for failed units.

Type II censoring has the significant advantage that you know in advance how many failure times your test will yield - this helps enormously when planning adequate tests. However, an open-ended random test time is generally impractical from a management point of view and this type of testing is rarely seen.

>Sometimes we don't even know the exact time of failure

##### Readout or Interval Data
Sometimes exact times of failure are not known; only an interval of time in which the failure occurred is recorded. This kind of data is called Readout or Interval data and the situation is shown in the figure below:

![Interval Data](http://www.itl.nist.gov/div898/handbook/apr/section1/gifs/multi.gif)

##### Multicensored Data

In the most general case, every unit observed yields exactly one of the following three types of information:

- a run-time if the unit did not fail while under observation
- an exact failure time
- an interval of time during which the unit failed.

The units may all have different run-times and/or readout intervals.

>Many special methods have been developed to handle censored data

##### How do we handle censored data?
Many statistical methods can be used to fit models and estimate failure rates, even with censored data. In later sections we will discuss the Kaplan-Meier approach, Probability Plotting, Hazard Plotting, Graphical Estimation, and Maximum Likelihood Estimation.

##### Separating out Failure Modes

Note that when a data set consists of failure times that can be sorted into several different failure modes, it is possible (and often necessary) to analyze and model each mode separately. Consider all failures due to modes other than the one being analyzed as censoring times, with the censored run-time equal to the time it failed due to the different (independent) failure mode. This is discussed further in the competing risk section and later analysis sections.

#### Lack of failures
>Failure data is needed to accurately assess and improve reliability - this poses problems when testing highly reliable parts

When fitting models and estimating failure rates from reliability data, the precision of the estimates (as measured by the width of the confidence intervals) tends to vary inversely with the square root of the number of failures observed - not the number of units on test or the length of the test. In other words, a test where 5 fail out of a total of 10 on test gives more information than a test with 1000 units but only 2 failures. 

Since the number of failures r is critical, and not the sample size n on test, it becomes increasingly difficult to assess the failure rates of highly reliable components. Parts like memory chips, that in typical use have failure rates measured in parts per million per thousand hours, will have few or no failures when tested for reasonable time periods with affordable sample sizes. This gives little or no information for accomplishing the two primary purposes of reliability testing, namely: 

- accurately assessing population failure rates
- obtaining failure mode information to feedback for product improvement.

>Testing at much higher than typical stresses can yield failures but models are then needed to relate these back to use stress

**How can tests be designed to overcome an expected lack of failures?**
The answer is to make failures occur by testing at much higher stresses than the units would normally see in their intended application. This creates a new problem: how can these failures at higher-than-normal stresses be related to what would be expected to happen over the course of many years at normal use stresses? The models that relate high stress reliability to normal use reliability are called [acceleration models](link).

### What is "physical acceleration" and how do we model it?
>When changing stress is equivalent to multiplying time to fail by a constant, we have true (physical) acceleration

**Physical Acceleration** (sometimes called **True Acceleration** or just **Acceleration**) means that operating a unit at high stress (i.e., higher temperature or voltage or humidity or duty cycle, etc.) produces the same failures that would occur at typical-use stresses, except that they happen much quicker. 

Failure may be due to mechanical fatigue, corrosion, chemical reaction, diffusion, migration, etc. These are the same causes of failure under normal stress; the time scale is simply different.

>An Acceleration Factor is the constant multiplier between the two stress levels

When there is true acceleration, changing stress is equivalent to transforming the time scale used to record when failures occur. The transformations commonly used are linear, which means that time-to-fail at high stress just has to be multiplied by a constant (the **acceleration factor**) to obtain the equivalent time-to-fail at use stress. 
We use the following notation: 
t~s~ = time-to-fail at stress	
t~u~ = corresponding time-to-fail at use
F~s~(t) = CDF at stress	
F~u~(t) = CDF at use
f~s~(t) = PDF at stress	
f~u~(t) = PDF at use
h~s~(t) = failure rate at stress	
h~u~(t) = failure rate at use

Then, an acceleration factor AF between stress and use means the following relationships hold: 

|Linear Acceleration Relationships||
------------------------|----------------------------------------
Time-to-Fail            |$t_u~=AF\times t_s$
Failure Probability	    |$F_u(t)=F_s(t/AF)$
Reliability	            |$R_u(t)=R_s(t/AF)$
PDF or Density Function	|$f_u(t)=\frac{1}{AF} f_s(\frac{t}{AF})$
Failure Rate	          |$h_u(t)=\frac{1}{AF} h_s(\frac{t}{AF})$

>Each failure mode has its own acceleration factor<br/>
Failure data should be separated by failure mode when analyzed, if acceleration is relevant<br/>
Probability plots of data from different stress cells have the same slope (if there is acceleration)

**Note**: Acceleration requires that there be a stress dependent physical process causing change or degradation that leads to failure. In general, different failure modes will be affected differently by stress and have different acceleration factors. Therefore, it is unlikely that a single acceleration factor will apply to more than one failure mechanism. In general, different failure modes will be affected differently by stress and have different acceleration factors. Separate out different types of failure when analyzing failure data. 
Also, a consequence of the linear acceleration relationships shown above (which follows directly from "true acceleration") is the following: 

>The Shape Parameter for the key life distribution models (Weibull, Lognormal) does not change for units operating under different stresses. Probability plots of data from different stress cells will line up roughly parallel.

These distributions and probability plotting will be discussed in later sections.
